## 第2章 神经网络的数学基础
- 关于类和标签的说明
	- 在机器学习中，分类问题中的某个**类别**叫做**类**（class）。
	- 数据点叫做**样本**（sample）。
	- 某个样本对应的类叫做**标签**（label）。
- **过拟合**是指机器学习模型在新数据上的性能往往比在训练数据上要差。
- 神经网络的数据表示
	- **张量**（tensor）是矩阵向任意纬度的推广[注意，张量的**维度**（dimension）通常叫做**轴**（axis）]
	- **标量**，scalar（0D张量）
	- **向量**，vector（1D张量）
	- **矩阵**，matrix（2D张量）
	- 3D张量与更高维张量
	- 关键属性
		- **轴的个数（阶）**，在Numpy等Python库中也叫张量的ndim。
		- **形状**。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。
		- **数据类型**（在Python库中通常叫做dtype）。这是张量中所包含数据的类型，例如，张量的类型可以是float32、unit8、float64等。
	- 现实世界中的数据张量
		- 向量数据：2D张量，形状为`(samples, features)`
		- 时间序列数据或序列数据：3D张量，形状为`(samples, timesteps, features)`。
		- 图像：4D张量，形状为`(samples, height, width, channels)` 或 `(samples, channels, height, width)`
		- 视频：5D张量，形状为`(samples, frames, height, width, channels)`或`(samples, frames, channels, height, width)`
- 神经网络的”引擎“：基于梯度的优化
	- 在神经网络示例中，每个神经层都用下述方法对输入的数据进行变换。
	- `output = relu(dot(w, input) + b)`
	- 在这个表达式中，w和b都是张量，均为该层的属性。他们被称为该层的**权重**（weight）或**可训练参数**（trainable parameter），分别对应kernel和bias属性。这些权重包含网络从观察训练数据中学到的信息。
	- 一开始这些权重矩阵取较小的随机值，这一步叫做**随机初始化**（random inittialization）。
	- 下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫做**训练**，也就是机器学习中的学习。
	- 上述过程发生在一个**训练循环**（training loop）内，其具体过程如下。必要时一致重复这些步骤。
		- (1)抽取训练样本x和对应目标y组成的数据批量。
		- (2)在x上运行网络[这一步叫作**向前传播**(forward pass)]，得到预测值y_pred。
		- (3)计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。
		- (4)更新网络的所有权重，使网络在这批数据上的损失略微下降。
	- 张量运算的导数：梯度
	- **梯度**（gradient）是张量运算的导数。他是导数这一概念向多元函数导数的推广。多元函数是以张量作为输入的函数。
	- 学习的过程：随机选区包含数据样本及其目标值的批量，并计算批量损失相对于网络参数的梯度。随后将网络参数沿着梯度的反方向稍稍移动（移动距离有学习率指定）。
	- 整个学习过程之所以死能够实现，是因为神经网络是一系列可微分的张量运算，因此可以利用求导的链式法则来得到梯度函数，这个函数将当前参数和当前数据批量映射为一个梯度值。
	- **损失**是在训练过程中需要最小化的量，因此，它应该能够衡量当前任务是否已成功解决。
	- **优化器**是试用损失梯度更新参数的具体方式，比如RMSProp优化器、带动量的随机梯度下降（SGD）等。