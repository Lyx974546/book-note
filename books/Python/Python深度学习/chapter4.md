## 第4章 机器学习基础
- 机器学习的四个分支
	- 监督学习
		- 序列生成（sequence generation)
		- 语法树预测(syntax tree prediction)
		- 目标检测(object detection)
		- 图像分割(image segmentation)
	- 无监督学习
		- 无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、数据压缩、数据去燥或更好地理解数据中的相关性。
		- 降维(dimensionality reduction)
		- 聚类(clustering)
	- 自监督学习
	- 强化学习
- 分类和回归术语表
	- **样本**(sample)或**输入**(input):进入模型的数据点。
	- **预测**(prediction)或**输出**(output):从模型出来的结果。
	- **目标**(targer):真实值。对于外部数据源，理想情况下，模型应该能够预测出目标。
	- **预测误差**(prediction error)或**损失值**(loss value):模型预测与目标之间的距离。
	- **类别**(class):分类问题中供选择的一直标签。
	- **标签**(label):分类问题中类别标注的具体例子。
	- **真值**(ground-truth)或**标注**(annotation):数据集所有目标，通常由人工收集。
	- **二分类**(binary classification): 一种分类任务，每个输入样本都应该被划分到两个互斥的类别中。
	- **多分类**(multiclass classification): 一种分类任务，每个输入样本都应该划分到两个以上的类别中。
	- **多标签分类**(multilabel classification): 一种分类任务，每个输入样本都可以分配多个标签。
	- **标量回归**(scalar regression):目标是连续标量值的任务。
	- **向量回归**(vector regression): 目标是一组连续值（比如一个连续向量）的任务。
	- **小批量**(mini-batch)或**批量**(batch):模型同时处理的一小部分样本（样本梳理为8~128）。样本数通常取2的幂，这样有便于GPU上的内存分配。训练时，小批量用来为模型权重计算一次梯度下降更新。
- 三种经典评估方法
	- 简单的留出验证
	- K折验证
	- 重复K折验证
- 选择模型评估方法时，需要注意一下几点
	- 数据代表性(data representativeness)
	- 时间箭头(the arrow of time)
	- 数据冗余(redundancy in your data)
- 神经网络的数据预处理
	- 数据预处理的目的是使原始数据更适于用神经网络处理，包括向量化、标准化、处理缺失值和特征提取。
- 特征工程
	- 特征工程(feature engineering)是指将数据输入模型之前，利用你自己关于数据和机器学习算法（这里指神经网络）的知识对数据进行硬编码的变换（不是模型学到的），以改善模型效果。
	- 多数情况下，一个机器学习模型无法从完全任意的数据中进行学习。呈现改模型的数据应该便于模型进行学习。
- 过拟合与欠拟合
	- 机器学习的根本问题是优化和泛化之间的对立。
	- **优化**(optimization)是指调节模型以在训练数据上得到最佳性能（即**机器学习**中的**学习**）
	- **泛化**(generalization)是指训练好的模型在前所未见的数据上的性能好坏。
	- 训练数据上的损失越小，测试数据上的损失也越小。这时的模型是**欠拟合**(underfit)的，即仍有改进的空间，网络还没有对训练的数据中所有相关模式建模。
	- 但在训练数据上迭代一定次数之后，泛化不再提高，验证指标先是不变，然后开始变差，即模型开始**过拟合**。
	- 降低过拟合的方法叫做**正则化**(regularization)。常见正则化方法：
		- 减小网络大小。防止过拟合的最简单方法就是减小模型大小，即减少模型中可学习参数个数（这由层数和每层的单元个数决定）。
		- 添加权重正则化。一种常见的降低过拟合的方法就是强制让模型权重只能取较小的值，从而限制模型的复杂度，这使得权重值的分布更加规则（regular）。这种方法叫做**权重正则化**（weight regularization），其实现方法是向网络损失函数中添加与交大权重相关的**成本**(cost)。这个成本有两种形式。
			- **L1正则化(L1 regularization)**:添加的成本与**权重系数的绝对值**[权重的**L1范数**(norm)]
			- **L2正则化(L2 regularization)**:添加的成本与**权重系数的平方**（权重的**L2范数**）成正比。神经网络的L2正则化也叫**权重衰减**(weight decay)。
		- 添加dropout正则化
- 机器学习的通用工作流程
	- 定义问题，收集数据集
	- 选择衡量成功的指标
	- 确定评估方法
	- 准备数据
	- 开发比基准更好的模型
	- 扩大模型规模：开发过拟合的模型
	- 模型正则化与调节超参数